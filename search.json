[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "The Conference on Reproducibility and Replicability in Economics and the Social Sciences is a series of virtual and in-person panels on the topics of reproducibility, replicability, and transparency in the social sciences. The purpose of scientific publishing is the dissemination of robust research findings, exposing them to the scrutiny of peers and other interested parties. Scientific articles should accurately and completely provide information on the origin and provenance of data and on the analytical and computational methods used. Yet in recent years, doubts about the adequacy of the information provided in scientific articles and their addenda have been voiced. The conferences will address the following topics: the initiation of research, the conduct of research, the preparation of research for publication, and the scrutiny after publication. Undergraduates, graduate students, and career researchers will be able to learn about best practices for transparent, reproducible, and scientifically sound research in the social sciences."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Events",
    "section": "",
    "text": "Upcoming and past webinars can be found here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact Us",
    "section": "",
    "text": "For more information, or if you are a presenter and have questions, please contact us.\nCRRESS is managed by co-PIs Lars Vilhuber and Aleksandr Michuda (Cornell University).\nThe organizing committee is composed of Vilhuber, Michuda, Ian Schmutte (UGA), and Marie Connolly (UQAM).\nSupport is provided by Sara Brooks (Cornell University) as well as the staff at the Cornell University ILR School."
  },
  {
    "objectID": "sessions/session1/session1_intro.html",
    "href": "sessions/session1/session1_intro.html",
    "title": "Session 1 - Institutional support: Should journals verify reproducibility?",
    "section": "",
    "text": "Different journals have different approaches towards enforcement of their data availability policies, ranging from a thorough and complete verification including running code and checking the output, to a cursory review of the files provided to make sure they appear satisfactory, to simply receiving the data and code package and archiving it on a website or a repository. What drives the choice of approach? What are the reasons behind such choices?\nIn this webinar, held on September 27th, 2022, and moderated by Lars Vilhuber, we had three panelists who are experts on this topic:\n\nGuido Imbens, Professor of Economics at the School of Humanities and Sciences; Senior Fellow at the Stanford Institute for Economic Policy Research; Coulter Family Faculty Fellow at Stanford University, and editor of Econometrica,\nTim Salmon, Professor of Economics at Southern Methodist University and the editor of Economic Inquiry, and\nToni Whited, Dale L. Dykema Professor of Business Administration at the Ross School of Business at the University of Michigan and editor-in-chief at the Journal of Financial Economics."
  },
  {
    "objectID": "sessions/session1/converted/tim_salmon/tim_salmon.html#background",
    "href": "sessions/session1/converted/tim_salmon/tim_salmon.html#background",
    "title": "Replication Packages for Journals: For and Against",
    "section": "Background",
    "text": "Background\nI have been on the editorial boards of many different journals for over 10 years. That experience, and my experience trying to publish in journals for much longer, has made me frequently question the editorial process, how to improve it and how journals can maintain high standards for work which they publish. In July of 2021, I took over as Editor of Economic Inquiry and was then in position to begin putting in place some policies which I thought would be beneficial in this regard. One of the first policies that I began working on was a policy requiring authors to share data and code related to papers published in the journal. I, of course, borrowed liberally from other journals which had already adopted such policies as there were many good models out there to borrow from. When the policy was finalized, we had chosen to fund a repository on OPENICPSR for both journals operated by the Western Economic Association International (Contemporary Economic Policy being the other journal) and establish a policy that requires all papers published by EI which include data analysis to publish a replication archive on that or a suitable alternative site. I had many discussions along the way to arrive at that policy and here I will explain some of the considerations which helped me to make the final choice."
  },
  {
    "objectID": "sessions/session1/converted/tim_salmon/tim_salmon.html#main-thoughts",
    "href": "sessions/session1/converted/tim_salmon/tim_salmon.html#main-thoughts",
    "title": "Replication Packages for Journals: For and Against",
    "section": "Main Thoughts",
    "text": "Main Thoughts\n\nWhy Should Journal Require Replication Packages?\nWe can first examine the case in favor of journals operating data archive sites like ours or in general of requiring authors to post replication packages which will allow others to reproduce their work. The main point behind this push for reproducible science is that such measures are necessary not just to maintain the credibility of individual research papers but to maintain the credibility of all academic research. There have been many examples of fraudulent work being published in academic journals over the years including many cases of researchers faking data. Two of the more famous incidences of this type of fraud were by Michael LaCour and Diedrik Stapel. In the case of LaCour, he was able to publish a paper in Science, supposedly the top journal across all disciplines for academic research, in 2016 which claimed to show that contact with a homosexual individual improved one‚Äôs support for gay marriage proposals.1 This was a blockbuster finding picked up by many news outlets. It was quite humiliating to many involved when it was later discovered that the data were faked. Diedrik Stapel is a repeat offender on this issue as he was able to publish many different studies in high quality journals on the basis of faked data.2 There are also other types of poor quality research that are fraudulent despite using real data which show up in journals as well. Among the more notorious offenders here would be Brian Wansink, the former head of a large research center at Cornell, who was also forced to retract many articles once the methods behind those articles were revealed.3 In his case, the data existed but he engaged in methods to achieve his results which involved, to quote Cornell's Provost at the time Michael Kotlikoff, \"misreporting of research data, problematic statistical techniques, failure to properly document and preserve research results, and inappropriate authorship.\"4 Many of the results from these papers had also been picked up in the popular press and so the findings of research misconduct here were quite public and embarrassing to all of the research community that allowed this work to publish. Many more examples of these problems can be found on https://retractionwatch.com/ and indeed the fact that such a website exists is a testament to the fact that far too much problematic research somehow makes its way to the pages of scientific journals.\nWe clearly need to do better and requiring more transparency in empirical work at journals is a good start. Facing requirements to provide the all of the underlying data, explicit details on methods for data collection and code for conducting the regressions would undoubtedly deter most of the cases discussed above and many other besides. This is because being required to produce the data and make it visible to others would often unmask the underlying fraud quickly and easily. There would also be a clear public record one could check to determine legitimacy of the work. Knowing it will be harder to pass through, one hopes fewer would try and when those few still try, it should be easier to uncover the problems and deal with them as necessary. Further, not only should these requirements reduce these egregious cases of fraud, which thankfully are not that wide spread, but they will force all authors to think very carefully through their empirical processes knowing that they will be publicly viewable. This increased scrutiny should hopefully improve the quality of all research published in our journals. Preventing cases of fraud while making the details of high quality research transparently available should be a substantial boost to the legitimacy of all of our work.\nIt is also important that journals have policies about data availability because the ability for future researchers to reproduce existing work is necessary for the advancement of science. In many cases, one research group may wish to build upon the work already published in a journal. A first step in that process is often reproducing the initial work so that the researchers can start from there and build up. Unfortunately, if these data availability policies are not in place it is often quite difficult for a set of researchers to back out exactly what others did from a published paper alone. In one case at my own journal, a paper was submitted which was attempting to do exactly this of building off of a previous paper published at the journal. The new paper's goal was to improve on the estimation process of the previous one. The problem is that the new researchers could not reproduce the original results and so their \"replication\" estimation generated a result not just quantitatively different from the original authors but qualitatively different in a very meaningful way as well. This makes it then difficult to evaluate whether their improvement to the original estimation approach yielded an improvement as it is unclear that they replicated the original one correctly. That is a problem for the researchers who previously published their work as it is harder for others to build on it and it is certainly frustrating for the later researchers who cannot replicate the prior work. Having replication packages accompanying published papers can resolve this problem quickly as researchers who wish to build off of the work of others can see exactly what they did to get those results without guessing and potentially failing to identify exactly what they did.\nA great example of the reason that replicating the work of others is often difficult is contained in Huntington-Klein et al (2021). This study examines the problem of replication at a deeper level than what journals usually engage in. The authors of this paper asked several teams of researchers to take the same raw data as two published papers and try to provide an answer to the same research question posed in those papers. This meant that the new researchers had to take the initial data, make all of the choices empirical researchers have to make about processing that data and specify a final regression to examine the issue. The results were that the original results often did not replicate. In some cases, the replication studies found a different sign on the key effect in question while in others, the magnitude and standard error of the effect were quite different. Importantly, in all cases, the final number of data points considered differed despite all studies starting with the same raw data with the same number of observations. The discrepancy in the final results may have been due to the fact that different research teams often made very different choices along the way to the final specification. And thus, to really know how a team of researchers arrived at a set of results, one really needs to know more than just what was the nature of the regression conducted but you need to know all the small steps along the way to get there. Without this detailed level of information, it can be impossible to really understand how two different studies arrived at different outcomes.\nIt is important at this point to distinguish between two very different, though related, goals of the data availability policies of journals and how data archives may be vetted by journals. The most commonly discussed check that journals may wish to perform about a replication archive is whether one can use the archive to reproduce the results in the paper. Such a certification verifies that indeed when code is run that the results of that code reproduce what is in the paper. This verification is valuable, but a certification that the authors can re-produce their own results is not really all that useful on its own. What the paper just discussed points out is that we also need the replication sets to provide all of the details regarding how the empirical analysis was performed so that future researchers can know exactly what the authors did. With this knowledge, future researchers can begin from more robust baselines regarding published work. Without this information, we run the risk of having many parallel research programs generating seemingly conflicting results with no way to clearly determine if the conflict is due to regression specifications, different choices in data processing, errors in data processing or something else along the research chain. When designing data availability policies, we need to keep both of these goals in mind and when certifying archives as being of high quality, we need to ensure that both of these goals can be achieved.\n\n\nWhy Shouldn't Journals Require Replication Packages?\nWhile I find the arguments above convincing regarding why journals should require replication packages, when I was contemplating putting one in place for EI, I did talk to many people who were of the opinion that journals should not be putting these requirements in place. It is worth examining their arguments against these policies to determine how convincing they are.\nThe first concern many would suggest about these archives is that if authors are required to post their data and their code for conducting their analysis, then others would be able to copy their work. Their concern is that the authors may have spent a great deal of time figuring out how to find the data involved, merge multiple data sets and clean them so that they work together. It may have also taken a great deal of time to implement the empirical methodology for the model in the paper. Many researchers may wish to keep that work for themselves so that they may continue to exploit that in future publications and do not want to allow others to make use of their efforts. At face value, this argument may seem somewhat convincing. While I had my own response to this, I have to say that the most convincing counter-argument against this line of thinking came from Guido Imbens in our panel discussion on this topic. He pointed out that allowing empirical researchers to hide their methods like this is similar to allowing theorists to publish theorems while keeping the proofs hidden. A theorist could mount the same argument that the proof may have taken a long time to work out, perhaps requiring the development of special techniques in the process and they may wish to be the only ones exploiting their methods in future work. We do not allow theorists to avoid providing proofs because we need to see verification that the theorems are valid. We do not simply trust them blindly. Yet empiricists who wish to hide their methods are asking journals to blindly trust them. That should not be how publishing works. Also, while yes, making your methods and data transparent may allow others to \"copy\" your work, the proper way to see that is that it allows others to build off of your work. Your work can now form the foundation of the work of others and have greater impact. I would argue that the possibility that it allows others to learn more from your work is in fact one of the main reasons why journals should be requiring these packages. It is not a downside.\nAnother common concern about journals requiring replication packages is the suggestion that these requirements place an undue burden on authors. This can be of particular concern to certain journals as putting such requirements in place could potentially decrease the number of submissions to the journal as authors decide to submit to peer journals without such requirements. Journals likely do need to weigh this concern when considering how stringent to make their data availability policy. It is worth noting that as more and more journals adopt these policies, authors will have fewer places to submit where they can avoid these requirements and so over time concerns over this issue should diminish. It is also worth considering as a journal editor whether you want to be among the last journals not enforcing these requirements. If you are, this will mean that all those people who do not want to engage in transparent research practices will submit to your journal. As an editor, do you want to be the recipient of those submissions? Perhaps not though that decision may depend on the peer journal group for a specific journal. For journals whose peers are not yet putting these policies in place, then even high quality authors might wish to avoid the burden if they have good alternatives. For journals whose peers mostly have these requirements, then being one of the few that do not poses significant risk to the journal of receiving work for which there is a reason the authors wish to avoid transparency. Different journal editors may examine this issue and come to different conclusions on the right policy for their journal at a specific point in time. For EI, we have had the policy in place for a little less than one year and based on the current data our total submissions are slightly lower than the previous few years at this point in the cycle.5 There are a few other possible explanations so it is not clearly attributable to this policy but the decrease is not at a problematic level even if the data policy is responsible for the entire decline.\nMy other view on the issue of a replication package being a burden on authors is that this is only true if authors wait until the end of the publication process to think about the reproducibility of their work. If authors have engaged in their work in a haphazard way prior to acceptance, then it can indeed be a substantial burden to go back and document all of the data manipulation that was done and script all of the regressions performed. If, however, authors begin thinking about these issues when they begin their research, there is no real burden and in fact I would argue that engaging in your research in a way from the beginning which will make the work replicable will actually save the authors time and allow them to do higher quality work. In my own work, I admit that early in my career I did much of my data work by hand. Then when I got a referee comment suggesting a different way to conduct a regression I would have to engage in some forensic econometrics to first back out what I actually did to get the prior result. This was wasted time and not the best way to do research. Now that I have all the analysis scripted, making changes like this is much faster and I do not have to wonder exactly how I created a variable or exactly which observations may have been dropped or why. All of that is in the scripting files from the beginning. As authors begin to expect to face these requirements and learn how to take this into account from the beginning of their analysis, the burden of providing a replication package upon acceptance of the paper diminishes substantially. I expect that these practices should be becoming more common in the profession and so the concern over this element should diminish with time. We can further diminish them by making sure that replicable research is brought into Ph.D.¬†training programs.\nA final notion that some suggested to me is that there is no need for journals to require replication packages. Individuals who want to provide their data can do so on their own sites and if there are professional incentives to do so perhaps in the form of these packages being seen as signals of high quality, everyone will do this anyway. Perhaps this could be true but most do not currently publicly archive replication files absent journal requirements. Were that to start, then it could be seen as a high-quality signal when someone does it which would mean that as journal editors we should be taking it into account in our decisions whether someone provides the data archives. If we do that, it is just a backdoor way to require replication archives but with a serious downside. If we make an accept decision based on an author saying that they will post an archive, after the paper is published authors could quickly pull that archive. Essentially, this approach is not an effective way of accomplishing the goals of research transparency. In order to ensure that the data remains available, it is best that journals maintain the archives for integrity of the process so that authors cannot manipulate the archive after the paper is published."
  },
  {
    "objectID": "sessions/session1/converted/tim_salmon/tim_salmon.html#conclusion",
    "href": "sessions/session1/converted/tim_salmon/tim_salmon.html#conclusion",
    "title": "Replication Packages for Journals: For and Against",
    "section": "Conclusion",
    "text": "Conclusion\nI believe quite strongly in the need for transparency in research. In order to preserve and maintain the integrity of all of academic research, we need to push for ever greater transparency in how research is done. That way when there are questions about the legitimacy of a claim, those questions can be quickly and easily addressed. This level of legitimacy is a benefit to us all. The main \"cost\" (if one sees it that way) would be that greater transparency limits the ability of people to publish ill-founded results. It is true that greater transparency does place greater requirements on researchers to engage in more careful and rigorous work which can survive the greater scrutiny possible with the increase to transparency. I see this as a clear benefit rather than as a cost.\nOf course, the path to this greater transparency norm will not be direct and not all journals will adopt the same standards at the same time. There are some journals leading in this direction, some following and some lagging behind. There are good reasons for different journals to be in each of those stages. As journals collectively move along this path it is important that we do so in ways that are not unduly burdensome on authors. This means that while requiring replicable archives is a valuable thing, it does not make sense for different journals to impose very different and idiosyncratic requirements about file structures and things like that such that when authors prepare a replication archive they must do a great deal of work to change it from a format suitable to one journal versus another. As a journal editor I appreciate the work done by others to establish clear guidelines on these issues which other journals can adopt as well to try to harmonize these requirements where we can."
  },
  {
    "objectID": "sessions/session1/converted/tim_salmon/tim_salmon.html#references",
    "href": "sessions/session1/converted/tim_salmon/tim_salmon.html#references",
    "title": "Replication Packages for Journals: For and Against",
    "section": "References",
    "text": "References\nHuntington-Klein, N., Arenas, A., Beam, E., Bertoni, M., Bloem, J.R., Burli, P. et al.¬†(2021) The influence of hidden researcher decisions in applied microeconomics. Economic Inquiry, 59: 944‚Äì 960. https://doi.org/10.1111/ecin.12992"
  },
  {
    "objectID": "sessions/session1/converted/toni_whited/toni_whited.html#introduction",
    "href": "sessions/session1/converted/toni_whited/toni_whited.html#introduction",
    "title": "Comments on Reproducibility in Finance and Economics",
    "section": "Introduction",
    "text": "Introduction\nReproducibility is defined as obtaining consistent results using the same data and code as the original study. Most of the discussion of reproducibility has centered around the many obvious benefits. Reproducible research advances knowledge for several reasons. It reduces the risk of errors. It also makes the processes that generate results more transparent. This second advantage has an important educational component, as it helps disseminate not just results but processes. However, reproducibility is not without costs. Good research procedures consume resources both in terms of a researcher‚Äôs own efforts and in terms of the involvement of arms-length parties in actually reproducing the research. This second cost is not just a time cost; it is pecuniary as well.\nThus, reproducibility is a good that is costly to produce and that has many positive externalities. Researchers internalize many of the benefits of reproducibility, especially in terms of research extendability and personal reputation. However, they do not internalize any of the benefits to the research community at large. Because reproducibility is costly, it is unlikely to be produced at a socially optimal rate by any individual researchers. Thus, the questions are the extent to which reproducibility should be subsidized and who should subsidize it. Should all research be reproduced by arms-length parties, and what are the least costly policies that facilitate reproducible research? The rest of this note is organized around policies regarding actual reproduction and proprietary data."
  },
  {
    "objectID": "sessions/session1/converted/toni_whited/toni_whited.html#code-data-and-arms-length-reproduction",
    "href": "sessions/session1/converted/toni_whited/toni_whited.html#code-data-and-arms-length-reproduction",
    "title": "Comments on Reproducibility in Finance and Economics",
    "section": "Code, Data, and Arms-Length Reproduction",
    "text": "Code, Data, and Arms-Length Reproduction\nOne low-cost and easily implementable set of policies that enhances the reproducibility of research is journals‚Äô data and code disclosure policies. In the age of inexpensive data storage and an abundance of public repositories, the costs of these policies are small, and the policies should be implemented. They impose some costs on researchers in terms of organizing data and code, but well-organized data and code are already an essential part of the research process, so these costs should be small.\nWhile simple to implement, this low-cost policy is not without non-pecuniary drawbacks for journals. The code and data can be incomplete, poorly documented, or unusable. Moreover, journal editors have to retract articles that, after publication, cannot be reproduced. In economics, these concerns have prompted journals to start arms-length reproduction of results. The benefit of this policy is primarily that authors and journals can be confident that the code submitted with an article actually works to reproduce the results.\nHowever, the pecuniary costs of this policy can be substantial. It is expensive for journals to hire data editors and well-trained research assistants, and many academic journals run on tight budgets. It is often time-consuming for authors to comply with reproducibility requirements. This last issue is particularly burdensome for authors who cannot afford research assistance.\nWhile the above issues involve costs, the following are more fundamental. Reproducibility policies give researchers incentives to do research that is easier to reproduce, thus restraining research innovation that requires either large data or intense computing. Most importantly, code that can run on data and reproduce results can still contain errors.\nThese arguments imply that while individual researchers are likely to underproduce reproducibility, it is also unlikely optimal for the progress of science that all research be reproduced before publication. Some papers, even those in the very best journals, rarely get read or cited, and the benefits of reproducing these papers are small.\nHowever, ex-ante, it is hard to know which papers will attract attention and which will not. One solution that lies between data and code disclosure and arms-length reproduction is verification. It is much less expensive to verify the contents of a replication package than to do an actual reproduction. Verification might consist of checking for the existence of replication instructions, an execution script, or either data or pseudo-data. This type of service could be provided by journals or other third parties, much as copy editors fix syntax and grammar errors before articles are submitted. At that point, reproducibility would be left up to the academic community, with the more important pieces of research being subject to greater scrutiny.\nA final issue with reproducibility is education. In economics and finance, students are not taught how to create reproducible research. An improvement that would go a long way toward improving the culture surrounding reproducibility would be to teach PhD students how to organize research projects and to write code in such a way that others can reproduce results easily. This type of education would lower the costs to individual researchers of making their own research reproducible."
  },
  {
    "objectID": "sessions/session1/converted/toni_whited/toni_whited.html#proprietary-data",
    "href": "sessions/session1/converted/toni_whited/toni_whited.html#proprietary-data",
    "title": "Comments on Reproducibility in Finance and Economics",
    "section": "Proprietary Data",
    "text": "Proprietary Data\nA possibly larger challenge for reproducibility than verification or arms-length execution of code is proprietary data. A clarification is necessary because not all types of data with restricted access are completely secret, that is, available only to the data provider and a researcher. For example, commercial data sets are not secret, just costly to obtain. Similarly, administrative datasets are not secret. They just require special permission. In contrast, proprietary data cannot be offered to the research community at large for the purposes of reproducing the results. So the question is whether journals should discourage the use of this type of data or require that verifiers have access to the data. Given the large number of studies using proprietary data, this issue is possibly more important than the issue of running code."
  },
  {
    "objectID": "sessions/session1/converted/toni_whited/toni_whited.html#conclusion",
    "href": "sessions/session1/converted/toni_whited/toni_whited.html#conclusion",
    "title": "Comments on Reproducibility in Finance and Economics",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the reproducibility of research is essential for the advancement of science. However, it is not without costs, so blanket statements that all research should be reproducible are not feasible. Instead, feasible policies include those that lower the costs for others to replicate research. Data and code disclosure is a low-cost policy that should be implemented widely. Verification of code and data packages is a slightly more costly option. Arms-length reproduction is a much more costly alternative. Finally, perhaps the most important issue that impedes reproducibility in finance and economics is the use of proprietary data."
  },
  {
    "objectID": "sessions/session2/session2_intro.html",
    "href": "sessions/session2/session2_intro.html",
    "title": "Session 2 - Replication and IRB",
    "section": "",
    "text": "Lorem ipsum dolor sit amet. Vel aliquid autem et deleniti deleniti qui labore vitae est soluta exercitationem est laborum culpa At numquam voluptas. Ab inventore sunt et ullam excepturi vel quod aspernatur aut consequatur voluptatum in modi perspiciatis. Eos repudiandae quod aut quia accusamus qui sunt deserunt sit officia ipsa eum soluta nesciunt qui voluptas velit ut quidem mollitia. Ea nostrum excepturi aut perspiciatis iusto qui porro adipisci.\nMolestiae facere aut voluptas beatae qui repudiandae iusto eos quibusdam culpa. Et nulla dolor in minima illo ad mollitia consequuntur eos odit ratione ut voluptatem eligendi ea unde ullam. Sed illo fuga eos doloremque deleniti ea mollitia voluptatibus nam officiis sint. Eos sunt molestias vel molestias alias id recusandae incidunt aut omnis culpa?\nAb minus consectetur ab eius quia non atque magnam est neque expedita aut voluptate asperiores in error consequatur et minima libero. Est deleniti voluptatem eos ratione reiciendis qui consequatur sunt et molestiae corrupti. Sit vitae quia ut suscipit necessitatibus et numquam nobis in nemo deserunt vel eius consequatur."
  },
  {
    "objectID": "sessions/session2/converted/paper4/paper4.html#summary",
    "href": "sessions/session2/converted/paper4/paper4.html#summary",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Summary",
    "text": "Summary\nLorem ipsum dolor sit amet. Eum accusamus expedita id totam laborum ea debitis natus est molestiae accusamus et alias quis sed repellendus omnis ad blanditiis incidunt. Et corrupti omnis qui autem fugit id natus exercitationem non nobis sunt aut reiciendis dolorem ad laboriosam sint. Cum eaque doloremque aut voluptatem quam ex molestiae aliquam.\nEx deserunt exercitationem sit reprehenderit blanditiis rem molestiae eveniet facilis quam non dolorum provident hic dolorem officia ex tempora sint. Est dolorum Quis aut esse modi aut quod molestias et illum quia provident omnis ut sunt dolor. Qui impedit totam qui nihil error et voluptate sint eum velit alias est commodi esse hic repudiandae error id aperiam officia."
  },
  {
    "objectID": "sessions/session2/converted/paper4/paper4.html#background",
    "href": "sessions/session2/converted/paper4/paper4.html#background",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Background",
    "text": "Background\nEt officia dolorem aut deserunt odio est eligendi corporis eum nobis debitis id dolorum nihil ea maiores nostrum aut quas iusto.1 Est quidem voluptas adipisci consectetur et animi autem et similique voluptatem ab expedita error qui repudiandae eligendi quo magnam distinction (Alesina et al.¬†2003; Posner 2004)."
  },
  {
    "objectID": "sessions/session2/converted/paper4/paper4.html#main-thoughts",
    "href": "sessions/session2/converted/paper4/paper4.html#main-thoughts",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Main Thoughts",
    "text": "Main Thoughts\nEt alias beataec eum omnis rerum aut eligendi eligendi qui dolor voluptate a debitis recusandae! Id galisum facere et mollitia enim et fugiat fugiat et dicta dolorem est atque nemo nam sunt natus id repellendus dolorum (Alesina et al.¬†2003)."
  },
  {
    "objectID": "sessions/session2/converted/paper4/paper4.html#conclusion",
    "href": "sessions/session2/converted/paper4/paper4.html#conclusion",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Conclusion",
    "text": "Conclusion\nEt officia dolorem aut deserunt odio est eligendi corporis eum nobis debitis id dolorum nihil ea maiores nostrum aut quas iusto.2 Est quidem voluptas adipisci consectetur et animi autem et similique voluptatem ab expedita error qui repudiandae eligendi quo magnam distinction (Alesina et al.¬†2003; Posner 2004)."
  },
  {
    "objectID": "sessions/session2/converted/paper4/paper4.html#references",
    "href": "sessions/session2/converted/paper4/paper4.html#references",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "References",
    "text": "References\nAlesina, Alberto et al.¬†2003. ‚ÄúFractionalization.‚Äù Journal of Economic growth 8(2): 155‚Äì94.\nPosner, Daniel N. 2004. ‚ÄúMeasuring Ethnic Fractionalization in Africa.‚Äù American journal of political science 48(4): 849‚Äì63."
  },
  {
    "objectID": "sessions/session2/converted/paper5/paper5.html#summary",
    "href": "sessions/session2/converted/paper5/paper5.html#summary",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Summary",
    "text": "Summary\nLorem ipsum dolor sit amet. Eum accusamus expedita id totam laborum ea debitis natus est molestiae accusamus et alias quis sed repellendus omnis ad blanditiis incidunt. Et corrupti omnis qui autem fugit id natus exercitationem non nobis sunt aut reiciendis dolorem ad laboriosam sint. Cum eaque doloremque aut voluptatem quam ex molestiae aliquam.\nEx deserunt exercitationem sit reprehenderit blanditiis rem molestiae eveniet facilis quam non dolorum provident hic dolorem officia ex tempora sint. Est dolorum Quis aut esse modi aut quod molestias et illum quia provident omnis ut sunt dolor. Qui impedit totam qui nihil error et voluptate sint eum velit alias est commodi esse hic repudiandae error id aperiam officia."
  },
  {
    "objectID": "sessions/session2/converted/paper5/paper5.html#background",
    "href": "sessions/session2/converted/paper5/paper5.html#background",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Background",
    "text": "Background\nEt officia dolorem aut deserunt odio est eligendi corporis eum nobis debitis id dolorum nihil ea maiores nostrum aut quas iusto.1 Est quidem voluptas adipisci consectetur et animi autem et similique voluptatem ab expedita error qui repudiandae eligendi quo magnam distinction (Alesina et al.¬†2003; Posner 2004)."
  },
  {
    "objectID": "sessions/session2/converted/paper5/paper5.html#main-thoughts",
    "href": "sessions/session2/converted/paper5/paper5.html#main-thoughts",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Main Thoughts",
    "text": "Main Thoughts\nEt alias beataec eum omnis rerum aut eligendi eligendi qui dolor voluptate a debitis recusandae! Id galisum facere et mollitia enim et fugiat fugiat et dicta dolorem est atque nemo nam sunt natus id repellendus dolorum (Alesina et al.¬†2003)."
  },
  {
    "objectID": "sessions/session2/converted/paper5/paper5.html#conclusion",
    "href": "sessions/session2/converted/paper5/paper5.html#conclusion",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "Conclusion",
    "text": "Conclusion\nEt officia dolorem aut deserunt odio est eligendi corporis eum nobis debitis id dolorum nihil ea maiores nostrum aut quas iusto.2 Est quidem voluptas adipisci consectetur et animi autem et similique voluptatem ab expedita error qui repudiandae eligendi quo magnam distinction (Alesina et al.¬†2003; Posner 2004)."
  },
  {
    "objectID": "sessions/session2/converted/paper5/paper5.html#references",
    "href": "sessions/session2/converted/paper5/paper5.html#references",
    "title": "Webinar 1 Write-up (Or put title here)",
    "section": "References",
    "text": "References\nAlesina, Alberto et al.¬†2003. ‚ÄúFractionalization.‚Äù Journal of Economic growth 8(2): 155‚Äì94.\nPosner, Daniel N. 2004. ‚ÄúMeasuring Ethnic Fractionalization in Africa.‚Äù American journal of political science 48(4): 849‚Äì63."
  },
  {
    "objectID": "sessions/session3/session3_intro.html",
    "href": "sessions/session3/session3_intro.html",
    "title": "Session 3 - Should teaching reproducibility be a part of undergraduate education or curriculum?",
    "section": "",
    "text": "This session was held at the Southern Economics Association meeting on November 20th, 2020. Panelists discussed teaching reproducibility (TIER Protocol), the involvement of undergraduates for replications based on restricted-access data, and other topics."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#introduction",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#introduction",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Introduction",
    "text": "Introduction\nThe scholarship of teaching and learning in economics documents multiple efforts to bring the quantitative dimension of our professional work closer to the undergraduate college curriculum.\nEconomics educators describing data-focused assignments and projects (Wolfe, 2020; Halliday, 2019; Wuthisatian and Thanetsunthorn, 2019; Marshall and Underwood, 2019; Mendez-Carbajo, 2015 & 2019) highlight the data-finding step of these projects. Even when the datasets are directly provided to the students, (e.g., Easton, 2020) the instructors emphasize the broader literacy dimensions of the assignments. However, there is neither professional consensus about how to build data-literacy skills (Wuthisatian and Thanetsunthorn, 2019) or much actual research on their mastery among economics students (Halliday, 2019).\nIn this chapter, we document baseline proficiency levels among undergraduate college students related to identifying data series and their sources. We also put forward an accessible pedagogical strategy to develop basic reproducibility skills.\nWe argue reproducibility should be part of the undergraduate curriculum in economics because it is a valuable professional skill to be developed throughout the curriculum by consistently citing the data sources used in economic arguments. We must instill the practice leading by example and enrolling the help of librarians"
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#expected-proficiencies",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#expected-proficiencies",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Expected Proficiencies",
    "text": "Expected Proficiencies\nThere is a natural overlap regarding the development of data-literacy skills between economics and library science: both disciplines value it and contribute to its development.\nThe two seminal descriptions of data literacy expected proficiencies among undergraduate students are provided by Hansen (2012) and Pothier and Condon (2019). The first of the seven broad competencies of economics majors named by Hansen directly address data provenance. It states: ‚ÄúAccess existing knowledge: [‚Ä¶] Track down economic data and data sources. Find information about the generation, construction, and meaning of economic data.‚Äù\nThe library science perspective provided by Pothier and Condon is articulated through seven expected data competencies of economics and business majors. The last one states: ‚ÄúData ethics: The principles of data ethics are built on data ownership, intellectual property rights, appropriate attribution and citation, and confidentiality and privacy issues involving human subjects.‚Äù\nThe utilitarian and ethical aspects of data reproducibility outlined above are bridged by the American Economic Association‚Äôs (AEA) (2020) Data and Code Availability Policy, which clearly states ‚ÄúAll source data used in the paper shall be cited, following the AEA Sample References.‚Äù However, the scholarship documenting the collaboration in this area between instructional economics faculty and librarians is limited. Neither the calls by economics instructors (McGrath and Tiemann, 1985; Li and Simonson, 2016; Mendez-Carbajo, 2016) nor the experiences documented by librarians (Wheatley, 2020; Wilhelm, 2021; Waggoner and Yates Habich, 2020) appear to have broad impact."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#evidence-of-broad-data-literacy-skills",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#evidence-of-broad-data-literacy-skills",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Evidence of Broad Data Literacy Skills",
    "text": "Evidence of Broad Data Literacy Skills\nMendez-Carbajo (2020) documents baseline levels of data literacy competency in several areas key to the accurate and ethical use of data for communication and decision-making among high school and college students.\nIn the online economic education module produced by the Federal Reserve Bank of St.¬†Louis ‚ÄúFRED Interactive: Information Literacy‚Äù, two separate groups of high school students (N= 450) and college students (N= 912) answer seven pre-test questions. The questions are mapped to the data literacy competencies described by both Pothier and Condon (2019) and Hansen (2012).\nThe analysis finds effectively identical levels of average baseline data literacy competency between high school and college students. However, it also documents much higher levels of perceived self-efficacy among college students than among high school students. In other words, college students are no more knowledgeable or skilled than high school students but are significantly more confident in their work. This finding highlights a major challenge for instructors working to develop the expected proficiencies identified in the literature: the average college student is unduly comfortable in their limited understanding of the primary sources of economic data."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#evidence-of-narrow-reproducibility-skills",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#evidence-of-narrow-reproducibility-skills",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Evidence of Narrow Reproducibility Skills",
    "text": "Evidence of Narrow Reproducibility Skills\nDuring the fall semester of 2020, we distributed a short online assignment to all 854 students enrolled in two different upper-division economics courses offered by a large public university in the United States.\nOn average, the students are slightly above 20 years of age, 49% identify themselves as female, 21% identify as non-White racial or ethnic minorities, and 92% report English is their native language. Academically, 87% of students are business, economics, or finance majors and hold a grade point average of 3.41. Also, 68% of students are currently enrolled in a statistics course required by their program and, on average, have previously completed more than one and a half economics courses.\nThe assignment had three sections:\n\nFirst, the students were directed to read a brief, 900-word, essay on how to create data citations with FRED¬Æ. This essay provided background on the value of good data citations for practitioners of economics and could be used as reference material for the next two sections of the assignment.\nSecond, the students were directed to read two short --under 600 words, economic essays. See them here and here. Each included a line graph of economic data. In the text, the authors referenced the data series and their sources while interpreting the quantitative information presented in the graph.\nThird, the students were asked to complete three tasks: identify the data series discussed in the essay; identify the sources of the data series discussed in the essay; and identify the missing elements of a data citation provided in the essay.\n\nThe assignment was completed in its entirety by 501 students. Table 1 reports our findings.\nTable 1. Data Literacy Skills\n\n\n\n\n\n\n\n\nScores, Misconceptions and Errors\nEssay A\nEssay B\n\n\n\n\nIdentifies Series Correctly\n0.57\n0.47\n\n\nIdentifies Sources Correctly\n0.21\n0.03\n\n\nIdentifies Incomplete Citation\n0.18\n-0.04\n\n\nCan‚Äôt Identify Sources\n0.05\n0.12\n\n\nConfuses Source with Distributor\n0.72\n0.73\n\n\nConsiders Citation to be Complete\n0.25\n0.40\n\n\n\nNote: Data Literacy Scores: ùëÜùëêùëúùëüùëí= (#ùê∂ùëúùëüùëüùëíùëêùë° ùê¥ùëõùë†ùë§ùëíùëüùë† ‚àí #ùêºùëõùëêùëúùëüùëüùëíùëêùë° ùê¥ùëõùë†ùë§ùëíùëüùë†) / (#ùê∂ùëúùëüùëüùëíùëêùë° ùê¥ùëõùë†ùë§ùëíùëüùë†)\nWe document very weak student data literacy competencies associated with narrow reproducibility skills. Data literacy scores related to correctly identifying the sources of the data or recognizing an incomplete data citation are very low. Moreover, we document a frequent misconception of confusing the data source with the distributor.\nThese findings have practical implications for instructors, whether they are librarians or economic educators. Our work suggests there is a substantial instructional opportunity to help students develop the ability to recognize data series and their sources. In that regard, disambiguating the roles of data distributors and data sources can potentially yield large benefits to students, who would be able to acquire a more sophisticated understanding of how data are created and made available."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#proposed-instructional-intervention",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#proposed-instructional-intervention",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Proposed Instructional Intervention",
    "text": "Proposed Instructional Intervention\nWe propose a broad instructional intervention for economics instructors reflecting the fact that correctly citing the data is a foundational literacy skill.\n\nLead students by example and consistently name the sources of all data referenced or used in your teaching.\nEmbed this practice in all your teaching, regardless of the type or subject of the course.\nEnroll the help of librarians by leveraging their ongoing instructional outreach on information literacy to include data citations.\n\nProficiency in identifying data sources is foundational to the development of reproducibility skills. The earlier and the more frequently students are exposed to best practices in data citations, the more effortlessly they will be able to adopt sophisticated professional replicability practices."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#conclusion",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#conclusion",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "Conclusion",
    "text": "Conclusion\nReproducibility should be part of the undergraduate curriculum in economics:\n\nIt is a valuable professional skill that shows the background work that goes into doing economic research. Citing the sources of the data makes research work more thorough.\nThis skill should be developed throughout the curriculum. This skill is not particular or exclusive to econometrics or statistics courses.\nThe first step is to consistently cite the data sources used in economic arguments. This includes data tables, plots, and in-text references.\nWe must instill the practice by leading by example. Economics educators should enroll the help of librarians in developing this skill among students."
  },
  {
    "objectID": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#references",
    "href": "sessions/session3/converted/mendez_carbajo/mendez_carbajo.html#references",
    "title": "Data Citations and Reproducibility\nin the Undergraduate Curriculum",
    "section": "References",
    "text": "References\nAmerican Economic Association. (2020). Data and Code Availability Policy. https://www.aeaweb.org/journals/data/data-code-policy.\nEaston, T. (2020). Teaching econometrics with data on coworker salaries and job satisfaction. International Review of Economics Education, 34, 100178. DOI 10.1016/j.iree.2020.100178.\nHalliday, S. D. (2019). Data literacy in economic development. The Journal of Economic Education, 50 (3), 284-298, DOI: 10.1080/00220485.2019.1618762\nHansen, W. L. (2012). An expected proficiencies approach to the economics major. In International handbook of teaching and learning economics, ed.¬†G. Hoyt and K. McGoldrick, 188‚Äì94. Cheltenham, UK and Northampton, MA: Edward Elgar.\nLi, I., and Simonson, R. D. (2016) The value of a redesigned program and capstone course in economics. International Review of Economics Education, 22, 48-58, DOI: 10.1016/j.iree.2016.05.001.\nMarshall, E. C., and Underwood, A. (2019). Writing in the discipline and reproducible methods: A process-oriented approach to teaching empirical undergraduate economics research. The Journal of Economic Education, 50 (1), 17-32. DOI: 10.1080/00220485.2018.1551100\nMcGrath, E. L., and Tiemann, T. K. (1985). Introducing empirical exercises into principles of economics. The Journal of Economic Education, 16 (2), 121-127. DOI: 10.1080/00220485.1985.10845107\nMendez-Carbajo, D. (2015). Visualizing data and the online FRED database. The Journal of Economic Education, 46 (4), 420-429. https://doi.org/10.1080/00220485.2015.1071222\nMendez-Carbajo, D. (2016). Quantitative reasoning and information literacy in economics. In Information Literacy: Research and Collaboration across Disciplines (pp.¬†305-322), Barbara D‚ÄôAngelo, Sandra Jamieson, Barry Maid, and J anice R. Walker (editors). Perspectives on Writing. Fort Collins, Colorado: WAC Clearinghouse and University of Colorado Press. https://wac.colostate.edu/books/infolit/chapter15.pdf\nMendez-Carbajo, D. (2019). Experiential learning in macroeconomics through FREDcast. International Review of Economic Education, 30 (1). DOI: 10.1016/j.iree.2018.05.004.\nMendez-Carbajo, D. (2020). Baseline competency and student self-efficacy in data literacy: Evidence from an online module. Journal of Business & Finance Librarianship, 25:3-4, 230-243. DOI: 10.1080/08963568.2020.1847551\nPothier, W., and Condon, P. (2019). Towards data literacy competencies: Business students, workforce needs, and the role of the librarian. Journal of Business and Finance Librarianship 25:3-4, 123-146. DOI: 10.1080/08963568.2019.1680189\nWaggoner, D., and Yates Habich, B. (2020). Collaboration is the key: faculty, librarian and Career Center professional unite for marketing class success. Journal of Business & Finance Librarianship, 25:1-2, 82-91. DOI: 10.1080/08963568.2020.1784658\nWilhelm, J. (2021). Joint venture: An exploratory case study of academic libraries‚Äô collaborations with career centers. Journal of Business & Finance Librarianship, 26:1-2, 16-31. DOI: 10.1080/08963568.2021.1893962\nWheatley, A., Chandler, M., and McKinnon, D. (2020). Collaborating with faculty on data awareness: A case study. Journal of Business & Finance Librarianship, 25:3-4, 281-290. DOI: 10.1080/08963568.2020.1847553\nWolfe, M. (2020). Integrating data analysis into an introductory macroeconomics course. International Review of Economics Education, 33, DOI: 10.1016/j.iree.2020100176\nWuthisatian, R., and Thanetsunthorn, N. (2019). Teaching macroeconomics with data: Materials for enhancing students‚Äô quantitative skills. International Review of Economics Education, 30, 100151. DOI 10.1016/j.iree.2018.11.001."
  },
  {
    "objectID": "sessions/session4/session4_intro.html",
    "href": "sessions/session4/session4_intro.html",
    "title": "Session 4: Reproducibility and confidential or proprietary data: can it be done?",
    "section": "",
    "text": "What happens to reproducibility when data are confidential or proprietary? Many journals can only ask that detailed access procedures be provided in a ReadMe file, but what mechanisms could be used to conduct computational reproducibility checks on such data? Should authors temporarily share their data with the journal for the purposes of reproducibility verification, even if they are not part of the public data replication package? Is it feasible to use a network of ‚Äúinsiders‚Äù to run code provided as part of a data replication package to assess reproducibility? Could a ‚Äúcertified run‚Äù be used?"
  },
  {
    "objectID": "sessions/session4/converted/guimaraes/guimaraes.html#background",
    "href": "sessions/session4/converted/guimaraes/guimaraes.html#background",
    "title": "Reproducibility with confidential data: The experience of BPLIM",
    "section": "Background",
    "text": "Background\nThe Banco de Portugal Microdata Laboratory (BPLIM) was established in 2016 with the primary goal of promoting external research on the Portuguese economy by making available data sets collected and maintained by Banco de Portugal (BdP). By making this information available to researchers from around the world, BdP aims to support the development of evidence-based policies and insights that can benefit the Portuguese economy and society. However, given that some of these data sets contain highly sensitive information BPLIM had to implement a data access solution that preserved the confidentiality of the data.\nThe common approach by other Research Data Centers that make confidential data available for research involves the provision of on-site access to accredited external researchers in a secure computing environment. However, in the case of BPLIM this approach was deemed undesirable for two reasons. First, because it would limit access to a handful of researchers who were able to come to the Bank's premises. Second, because there were still concerns that a breach of confidentiality might occur if individuals from outside the bank could gain access to original data sets that contained confidential information.\nAfter an internal debate at the Bank it was decided that the solution to be adopted by BPLIM had to be based on the following principles:\n- access free of charge and only for scientific purposes;\n- all data should be analyzed on the servers of the Bank;\n- external researchers were granted remote access to the server;\n- confidential datasets placed on the server had to always be perturbed/masked;\n- researchers could always ask BPLIM staff to run their scripts on the original data.\nThe general workflow defined for data access at BPLIM was the following. After a research project is approved and the external researchers are accredited an account is opened on the BPLIM external server. External researchers gain access to a computing environment that does not allow users to transfer files to and from the server. They have access to a restricted area where standard software such as Stata, R, Julia and Python are available. Since there is no connection to the internet, installation of specific packages has to be requested from the staff. The datasets for the project are placed in a read-only folder. For the confidential datasets what is placed in the account of the researcher are perturbed versions of the data (noise is added to the original data). The researcher implements all scripts based on the data he/she has available and produces the (non-valid) outputs required for the project. Once researchers complete this task, they can ask BPLIM staff to rerun their scripts, this time using the original confidential data. For this process to be successful BPLIM staff must first run the scripts using the same data as the researcher to verify that the scripts written by the researchers reproduce exactly the outputs (typically graphs and/or tables). This process is done in a different server (BPLIM internal server). Only upon completion of this first step can BPLIM staff modify the scripts, this time to read the original data and regenerate the intended outputs . These outputs are then subject to standard output control checks for confidential data and delivered to the researcher."
  },
  {
    "objectID": "sessions/session4/converted/guimaraes/guimaraes.html#main-thoughts",
    "href": "sessions/session4/converted/guimaraes/guimaraes.html#main-thoughts",
    "title": "Reproducibility with confidential data: The experience of BPLIM",
    "section": "Main Thoughts",
    "text": "Main Thoughts\nOver time we have come to realize that this somewhat cumbersome process of running the code thrice, first by the researcher on the perturbed data, second by BPLIM staff again on the perturbed data, and finally, on the original data was in fact an exercise on reproducibility. Even though the reproducibility check was on the perturbed data it was already a very good assurance that a reproducibility check would hold on the original data.\nWe realized that a great deal of our work involved reproducing the results obtained by the researchers with the perturbed data and that led us to look at ways to improve our workflow. It became obvious that the process could be streamlined and would be more efficient if researchers adhered to the best practices on reproducibility. Hence, as part of our strategy, we have decided to raise awareness of our researchers to the need of implementing good practices on reproducible research. We have been doing this by several means. For example, we have held practical workshops which are designed to enhance the skills of our researchers. For these workshops we invite leading experts to present best practices and recent developments on data analysis. We also provide direct advice to the researchers, prepare templates and documentation, and make available tools that facilitate the analysis of our datasets (particularly for more complex tasks such as building a panel or calculation of specific variables).\nOn the other end, it was also obvious that there was margin for improvement on our work sequence. One possible improvement was the assurance that the computing environment used by the researcher on BPLIM's external server was identical to that used by BPLIM staff when reproducing the code. Thus, for the case of researchers that work with open-source software, we have been incentivizing researchers to work with Singularity containers. This facilitates our work because we are sure that our reproducibility check is implemented in the same self-contained environment that was used by the researcher. Researchers that use Stata can resort to containers but in that case, it is easier to control the environment because we install all packages on a folder that is specific to each project and have developed tools that facilitate comparison of the Stata ado files across environments. (all tools are publicly available and can be found at https://github.com/BPLIM/Tools/tree/master/ados/General.)\nMore recently, we have worked on shifting the burden of the reproducibility check to the researcher itself. We are developing an application that we are presently testing with a select number of researchers. The application is targeted mainly at researchers that use BPLIM's (perturbed) confidential datasets but we hope to eventually convince all other users to take advantage of it. To illustrate, we provide a screenshot of the application:\n\nFigure - BPLIM Replication tool ‚Äì selecting input files\nBefore requesting a replication from BPLIM using the original data the researcher must first validate his/her code by successfully submitting the scripts through BPLIM's Replication application. The process involves selecting the main script as well as all the required dependencies created by the researcher. The folder structure used by the researcher is replicated and the BPLIM datasets have to be read from the (read-only) data folder. All intermediary output files must be created during the replication (it is however possible to start from an intermediary output file. In that case, the intermediary file must have been validated in a prior run. BPLIM will then copy the file to the (read-only) data folder.).\n\nFigure - The BPLIM Replication tool - finished task\nThe researcher then uses the application to run the code (see Figure 2 for a completed run). The code must run from top-to-bottom and produce no errors. If the run is successful, then implementation on the original data requires only that BPLIM staff changes the relative paths to the data folder and rerun the code. A side advantage of this process is that it automatically produces a replication package for the researcher. Stored in the folder are all replication scripts, the output files, as well as two additional files, one fully characterizing the software environment, and another json file containing a listing of all scripts used in the replication. If we add the definition file used to produce the container (or a listing of all packages and respective versions) then we have a full replication package (except for the data)."
  },
  {
    "objectID": "sessions/session4/converted/guimaraes/guimaraes.html#conclusion",
    "href": "sessions/session4/converted/guimaraes/guimaraes.html#conclusion",
    "title": "Reproducibility with confidential data: The experience of BPLIM",
    "section": "Conclusion",
    "text": "Conclusion\nOur goal at BPLIM is to make sure that all researchers create their replication packages as an integrated part of their research process. The fact that we are the ones running the code on the original data should be seen as an opportunity to request that researchers make reproducible code while implementing their research.\nIn the ideal situation that we envision, researchers download a template for the definition file of the Singularity container, customize that template by adding and testing the packages they need, and share with us the definition file. Based on that definition file we build the container for the project and make it available on our external server. The researcher then uses the container to implement the analysis and when he/she is ready to obtain results based on the original data he must first validate the scripts using our application. The researcher can go through this process multiple times and each time a replication package will be created. Apart from the data, all files can be publicly shared, and the replication package created at BPLIM should be easily customized for submission to any data editor.\nWe are already implementing this solution for all new projects that use confidential data. However, we hope that over time we can convince all researchers at BPLIM to work with Singularity containers and go through the same validation steps that are needed for projects that deal with confidential data.\nProjects that are implemented in BPLIM have additional advantages when it comes to reproducibility. First, because all BPLIM datasets are versioned and registered with a Digital Object Identifier (DOI) we are sure that the original data is exactly identified. Second, the computing environment is stable, and the software packages used by researchers are specific to each project. Finally, if external researchers have used BPLIM confidential data then there is an assurance that their code was reproduced at some point.\nUltimately, whether the scientific work is reproducible depends on the researchers. But we hope that integrating reproducibility into the research process with confidential data, provides a way to alleviate the inconvenience of third parties that cannot access the original data and want to verify reproducibility of the results."
  }
]